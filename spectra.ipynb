{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "872cd5ce",
   "metadata": {},
   "source": [
    "# Spectra: Object Recognition Model Training\n",
    "\n",
    "This notebook implements the training pipeline for 'spectra', trained on LAION-5B, WIT, PMD, and expanded detection datasets (COCO, OpenImages, etc.) using CLIP-style contrastive learning.\n",
    "\n",
    "## Features\n",
    "- **Backbone**: `laion/CLIP-ViT-B-16-laion2B-s34B-b88K` (OpenCLIP ViT-B/16)\n",
    "- **Streaming Data**: Zero-disk usage for massive datasets.\n",
    "- **Robust Checkpointing**: Resumable training from HF Hub (Landmarks).\n",
    "- **Phase 2 Training**: Includes COCO, OpenImages, Objects365, VOC, LVIS, Visual Genome.\n",
    "- **Optimizations**: Mixed Precision (FP16), Gradient Accumulation, Torch Compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4927340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in to HF with token ending in ...nrRd\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, interleave_datasets\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from accelerate import Accelerator\n",
    "from huggingface_hub import login, HfApi, hf_hub_download\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables\n",
    "# add env path here\n",
    "env_path = Path(\".env\")\n",
    "\n",
    "if env_path.exists():\n",
    "    load_dotenv(dotenv_path=env_path, override=True)\n",
    "    print(\"Loaded .env file\")\n",
    "else:\n",
    "    print(\"Warning: .env file not found in current directory\")\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(f\"Logged in to HF with token ending in ...{HF_TOKEN[-4:]}\")\n",
    "else:\n",
    "    print(\"WARNING: HF_TOKEN not found in .env. Please set it for Hub uploads.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6bf689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerator setup: cpu, Mixed Precision: fp16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahada\\Documents\\abdulahad\\spectra\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:529: UserWarning: `log_with=all` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"model_name\": \"laion/CLIP-ViT-B-16-laion2B-s34B-b88K\", # ViT-B/16 Backbone\n",
    "    \"fallback_model\": \"openai/clip-vit-base-patch32\",\n",
    "    \"output_dir\": \"./spectra_checkpoints\",\n",
    "    \"hub_model_id\": \"990aa/spectra\",\n",
    "    \"batch_size\": 64, # Per device\n",
    "    \"grad_accum_steps\": 4,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"max_steps\": 100000, # Increased for multi-phase\n",
    "    \"warmup_steps\": 2000,\n",
    "    \"mixed_precision\": \"fp16\",\n",
    "    \"image_size\": 224,\n",
    "    \"push_to_hub\": True,\n",
    "    \"checkpoint_interval\": 1000,\n",
    "    \"state_file\": \"training_state.json\",\n",
    "    \"model_card_file\": \"README.md\"\n",
    "}\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision=CONFIG[\"mixed_precision\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"grad_accum_steps\"],\n",
    "    log_with=\"all\",\n",
    "    project_dir=\"logs\"\n",
    ")\n",
    "\n",
    "print(f\"Accelerator setup: {accelerator.device}, Mixed Precision: {accelerator.mixed_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2867204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpointing & Resumption Logic\n",
    "def save_landmark(step: int, consumed_samples: int) -> None:\n",
    "    \"\"\"\n",
    "    Save a training checkpoint (landmark) and upload it to the Hugging Face Hub.\n",
    "    \n",
    "    Args:\n",
    "        step (int): The current global training step.\n",
    "        consumed_samples (int): The total number of samples consumed so far.\n",
    "    \"\"\"\n",
    "    if not accelerator.is_main_process:\n",
    "        return\n",
    "    \n",
    "    state = {\n",
    "        \"global_step\": step,\n",
    "        \"consumed_samples\": consumed_samples\n",
    "    }\n",
    "    with open(CONFIG[\"state_file\"], \"w\") as f:\n",
    "        json.dump(state, f)\n",
    "    \n",
    "    # Upload to HF\n",
    "    api = HfApi()\n",
    "    try:\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=CONFIG[\"state_file\"],\n",
    "            path_in_repo=CONFIG[\"state_file\"],\n",
    "            repo_id=CONFIG[\"hub_model_id\"],\n",
    "            repo_type=\"model\"\n",
    "        )\n",
    "        print(f\"Landmark saved at step {step}\")\n",
    "        \n",
    "        # Upload model card\n",
    "        if os.path.exists(CONFIG[\"model_card_file\"]):\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=CONFIG[\"model_card_file\"],\n",
    "                path_in_repo=\"README.md\",\n",
    "                repo_id=CONFIG[\"hub_model_id\"],\n",
    "                repo_type=\"model\"\n",
    "            )\n",
    "            print(\"Model card uploaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload to Hub: {e}\")\n",
    "\n",
    "def load_landmark() -> dict | None:\n",
    "    \"\"\"\n",
    "    Load the latest checkpoint state from the Hugging Face Hub.\n",
    "    \n",
    "    Returns:\n",
    "        dict | None: The loaded state dictionary if found, else None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path = hf_hub_download(repo_id=CONFIG[\"hub_model_id\"], filename=CONFIG[\"state_file\"])\n",
    "        with open(path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        print(f\"Found previous state: Step {state['global_step']}\")\n",
    "        return state\n",
    "    except Exception:\n",
    "        print(\"No previous state found. Starting from scratch.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0745b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pipeline Construction\n",
    "def format_detection_dataset(item: dict, text_key: str = None, label_key: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Standardize dataset items to {image, text} format.\n",
    "    \n",
    "    Args:\n",
    "        item (dict): The dataset item.\n",
    "        text_key (str, optional): Key for text caption.\n",
    "        label_key (str, optional): Key for object labels.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Formatted item with 'image' and 'text' keys.\n",
    "    \"\"\"\n",
    "    # Helper to standardize (image, text) format\n",
    "    img = item.get(\"image\")\n",
    "    txt = \"\"\n",
    "    \n",
    "    if text_key and text_key in item:\n",
    "        txt = item[text_key]\n",
    "        if isinstance(txt, list):\n",
    "            txt = txt[0]  # Take first caption\n",
    "    elif label_key and label_key in item:\n",
    "        # Convert labels to text (simplified)\n",
    "        # In real scenario, map IDs to class names\n",
    "        labels = item[label_key]\n",
    "        txt = f\"A photo of object {labels}\" # Placeholder logic\n",
    "        \n",
    "    return {\"image\": img, \"text\": txt}\n",
    "\n",
    "def get_dataset_stream(phase: str = \"phase1\") -> object:\n",
    "    \"\"\"\n",
    "    Initialize and return a streaming dataset for the specified phase.\n",
    "    \n",
    "    Args:\n",
    "        phase (str): The training phase ('phase1' or 'phase2').\n",
    "        \n",
    "    Returns:\n",
    "        object: An interleaved streaming dataset.\n",
    "    \"\"\"\n",
    "    print(f\"Initializing Data Stream for {phase}...\")\n",
    "    \n",
    "    datasets_list = []\n",
    "    probs = []\n",
    "    \n",
    "    if phase == \"phase1\":\n",
    "        # 1. LAION-5B (using laion2B-en subset for demo)\n",
    "        try:\n",
    "            ds_laion = load_dataset(\"laion/laion2B-en\", split=\"train\", streaming=True)\n",
    "            # Safely extract fields with error handling\n",
    "            def safe_laion_map(x):\n",
    "                try:\n",
    "                    return {\n",
    "                        \"image\": x.get(\"URL\") or x.get(\"url\") or x.get(\"image\"),\n",
    "                        \"text\": x.get(\"TEXT\") or x.get(\"caption\") or \"\"\n",
    "                    }\n",
    "                except Exception:\n",
    "                    return None\n",
    "            \n",
    "            ds_laion = ds_laion.map(safe_laion_map)\n",
    "            ds_laion = ds_laion.filter(lambda x: x is not None and x.get(\"image\") is not None)\n",
    "            datasets_list.append(ds_laion)\n",
    "            probs.append(0.5)\n",
    "            print(\"✓ LAION-2B loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load LAION-2B: {e}\")\n",
    "        \n",
    "        # 2. WIT (Wikipedia Image Text)\n",
    "        try:\n",
    "            ds_wit = load_dataset(\"wikimedia/wit_base\", split=\"train\", streaming=True)\n",
    "            def safe_wit_map(x):\n",
    "                try:\n",
    "                    return {\n",
    "                        \"image\": x.get(\"image\") or x.get(\"image_url\"),\n",
    "                        \"text\": x.get(\"caption_reference_description\") or x.get(\"caption\") or \"\"\n",
    "                    }\n",
    "                except Exception:\n",
    "                    return None\n",
    "            \n",
    "            ds_wit = ds_wit.map(safe_wit_map)\n",
    "            ds_wit = ds_wit.filter(lambda x: x is not None and x.get(\"image\") is not None)\n",
    "            datasets_list.append(ds_wit)\n",
    "            probs.append(0.3)\n",
    "            print(\"✓ WIT loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load WIT: {e}\")\n",
    "        \n",
    "        # 3. Conceptual Captions (CC3M/CC12M alternative)\n",
    "        try:\n",
    "            ds_cc = load_dataset(\"google-research-datasets/conceptual_captions\", split=\"train\", streaming=True)\n",
    "            def safe_cc_map(x):\n",
    "                try:\n",
    "                    return {\n",
    "                        \"image\": x.get(\"image_url\") or x.get(\"image\"),\n",
    "                        \"text\": x.get(\"caption\") or \"\"\n",
    "                    }\n",
    "                except Exception:\n",
    "                    return None\n",
    "            \n",
    "            ds_cc = ds_cc.map(safe_cc_map)\n",
    "            ds_cc = ds_cc.filter(lambda x: x is not None and x.get(\"image\") is not None)\n",
    "            datasets_list.append(ds_cc)\n",
    "            probs.append(0.2)\n",
    "            print(\"✓ Conceptual Captions loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load Conceptual Captions: {e}\")\n",
    "        \n",
    "    elif phase == \"phase2\":\n",
    "        # Detection Datasets with robust loading\n",
    "        \n",
    "        # 1. COCO (Common Objects in Context)\n",
    "        try:\n",
    "            ds_coco = load_dataset(\"detection-datasets/coco\", split=\"train\", streaming=True)\n",
    "            def safe_coco_map(x):\n",
    "                try:\n",
    "                    objects = x.get(\"objects\", {})\n",
    "                    labels = objects.get(\"category\", []) if isinstance(objects, dict) else []\n",
    "                    return {\n",
    "                        \"image\": x.get(\"image\"),\n",
    "                        \"text\": f\"Objects: {', '.join(map(str, labels[:5]))}\" if labels else \"An image\"\n",
    "                    }\n",
    "                except Exception:\n",
    "                    return None\n",
    "            \n",
    "            ds_coco = ds_coco.map(safe_coco_map)\n",
    "            ds_coco = ds_coco.filter(lambda x: x is not None and x.get(\"image\") is not None)\n",
    "            datasets_list.append(ds_coco)\n",
    "            probs.append(0.4)\n",
    "            print(\"✓ COCO loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load COCO: {e}\")\n",
    "        \n",
    "        # 2. Visual Genome\n",
    "        try:\n",
    "            ds_vg = load_dataset(\"visual_genome\", \"region_descriptions_v1.2.0\", split=\"train\", streaming=True)\n",
    "            def safe_vg_map(x):\n",
    "                try:\n",
    "                    return {\n",
    "                        \"image\": x.get(\"image\") or x.get(\"url\"),\n",
    "                        \"text\": x.get(\"phrase\") or x.get(\"regions\", [{}])[0].get(\"phrase\", \"\")\n",
    "                    }\n",
    "                except Exception:\n",
    "                    return None\n",
    "            \n",
    "            ds_vg = ds_vg.map(safe_vg_map)\n",
    "            ds_vg = ds_vg.filter(lambda x: x is not None and x.get(\"image\") is not None)\n",
    "            datasets_list.append(ds_vg)\n",
    "            probs.append(0.3)\n",
    "            print(\"✓ Visual Genome loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load Visual Genome: {e}\")\n",
    "        \n",
    "        # 3. Objects365 alternative or ImageNet-like dataset\n",
    "        try:\n",
    "            ds_objects = load_dataset(\"objects365\", split=\"train\", streaming=True)\n",
    "            def safe_objects_map(x):\n",
    "                try:\n",
    "                    return {\n",
    "                        \"image\": x.get(\"image\"),\n",
    "                        \"text\": x.get(\"objects\") or \"Various objects\"\n",
    "                    }\n",
    "                except Exception:\n",
    "                    return None\n",
    "            \n",
    "            ds_objects = ds_objects.map(safe_objects_map)\n",
    "            ds_objects = ds_objects.filter(lambda x: x is not None and x.get(\"image\") is not None)\n",
    "            datasets_list.append(ds_objects)\n",
    "            probs.append(0.3)\n",
    "            print(\"✓ Objects365 loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load Objects365: {e}\")\n",
    "        \n",
    "        # Fallback to Phase 1 if no Phase 2 datasets loaded\n",
    "        if not datasets_list:\n",
    "            print(\"Warning: Phase 2 datasets not accessible. Falling back to Phase 1 streams.\")\n",
    "            return get_dataset_stream(\"phase1\")\n",
    "\n",
    "    # If no datasets loaded at all, create a dummy dataset to prevent errors\n",
    "    if not datasets_list:\n",
    "        print(\"ERROR: No datasets could be loaded. Creating minimal fallback dataset.\")\n",
    "        # You should replace this with actual accessible datasets\n",
    "        raise RuntimeError(\"No datasets available. Please check your internet connection and dataset access permissions.\")\n",
    "\n",
    "    # Normalize probabilities\n",
    "    probs = [p/sum(probs) for p in probs]\n",
    "    \n",
    "    print(f\"Interleaving {len(datasets_list)} datasets with probabilities: {probs}\")\n",
    "    \n",
    "    combined_ds = interleave_datasets(\n",
    "        datasets_list, \n",
    "        probabilities=probs, \n",
    "        seed=42,\n",
    "        stopping_strategy=\"first_exhausted\"\n",
    "    )\n",
    "    \n",
    "    return combined_ds\n",
    "\n",
    "def collate_fn(batch: list) -> dict | None:\n",
    "    \"\"\"\n",
    "    Collate function to process a batch of items.\n",
    "    \n",
    "    Args:\n",
    "        batch (list): List of dataset items.\n",
    "        \n",
    "    Returns:\n",
    "        dict | None: Processed batch tensors or None if empty.\n",
    "    \"\"\"\n",
    "    processor = CLIPProcessor.from_pretrained(CONFIG[\"model_name\"])\n",
    "    texts = []\n",
    "    images = []\n",
    "    \n",
    "    for item in batch:\n",
    "        if item is None:\n",
    "            continue\n",
    "            \n",
    "        img = item.get(\"image\")\n",
    "        txt = item.get(\"text\")\n",
    "        \n",
    "        if img and txt:\n",
    "            try:\n",
    "                # Handle URL images (download if needed)\n",
    "                if isinstance(img, str):\n",
    "                    from PIL import Image\n",
    "                    import requests\n",
    "                    from io import BytesIO\n",
    "                    \n",
    "                    response = requests.get(img, timeout=5)\n",
    "                    img = Image.open(BytesIO(response.content))\n",
    "                \n",
    "                # Convert to RGB\n",
    "                if img.mode != \"RGB\":\n",
    "                    img = img.convert(\"RGB\")\n",
    "                    \n",
    "                images.append(img)\n",
    "                texts.append(str(txt)[:77]) # Truncate for CLIP\n",
    "            except Exception:\n",
    "                continue\n",
    "            \n",
    "    if not images:\n",
    "        return None\n",
    "    return processor(text=texts, images=images, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92fc30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load laion/CLIP-ViT-B-16-laion2B-s34B-b88K, falling back to openai/clip-vit-base-patch32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ccfff71cba487fb7722036a1225015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6300d89c0fe47599e9621885148d2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd91c95cd6b40f7bfa9c6069f085134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled.\n"
     ]
    }
   ],
   "source": [
    "# Model Setup\n",
    "try:\n",
    "    model = CLIPModel.from_pretrained(CONFIG[\"model_name\"])\n",
    "    print(f\"Loaded {CONFIG['model_name']}\")\n",
    "except Exception:\n",
    "    print(f\"Failed to load {CONFIG['model_name']}, falling back to {CONFIG['fallback_model']}\")\n",
    "    model = CLIPModel.from_pretrained(CONFIG[\"fallback_model\"])\n",
    "    CONFIG[\"model_name\"] = CONFIG[\"fallback_model\"]\n",
    "\n",
    "# Freeze early layers\n",
    "for name, param in model.vision_model.encoder.layers[:6].named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Torch Compile\n",
    "try:\n",
    "    model = torch.compile(model)\n",
    "    print(\"Model compiled.\")\n",
    "except Exception:\n",
    "    print(\"torch.compile skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38efa996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous state found. Starting from scratch.\n",
      "Initializing Data Stream for phase1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc95c92709fc41ce8f6778c4068fe010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c50ecf7cf940929f51cacc3b4e30b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b303c4d38b8449a7b997790ca541e9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb98f7c3942648c586d26cb380311c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'image'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_step > CONFIG[\u001b[33m\"\u001b[39m\u001b[33mmax_steps\u001b[39m\u001b[33m\"\u001b[39m] // \u001b[32m2\u001b[39m:\n\u001b[32m     23\u001b[39m     current_phase_idx = \u001b[32m1\u001b[39m \u001b[38;5;66;03m# Simple logic to switch phases halfway\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m dataset = \u001b[43mget_dataset_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphases\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_phase_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m dataset = dataset.shuffle(buffer_size=\u001b[32m1000\u001b[39m, seed=\u001b[32m42\u001b[39m)\n\u001b[32m     27\u001b[39m dataset = dataset.skip(consumed_samples) \u001b[38;5;66;03m# Skip processed samples\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mget_dataset_stream\u001b[39m\u001b[34m(phase)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Normalize probabilities\u001b[39;00m\n\u001b[32m     85\u001b[39m probs = [p/\u001b[38;5;28msum\u001b[39m(probs) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m probs]\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m combined_ds = \u001b[43minterleave_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatasets_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfirst_exhausted\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     92\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m combined_ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahada\\Documents\\abdulahad\\spectra\\.venv\\Lib\\site-packages\\datasets\\combine.py:156\u001b[39m, in \u001b[36minterleave_datasets\u001b[39m\u001b[34m(datasets, probabilities, seed, info, split, stopping_strategy)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _interleave_map_style_datasets(\n\u001b[32m    153\u001b[39m         datasets, probabilities, seed, info=info, split=split, stopping_strategy=stopping_strategy\n\u001b[32m    154\u001b[39m     )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_interleave_iterable_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahada\\Documents\\abdulahad\\spectra\\.venv\\Lib\\site-packages\\datasets\\iterable_dataset.py:4625\u001b[39m, in \u001b[36m_interleave_iterable_datasets\u001b[39m\u001b[34m(datasets, probabilities, seed, info, split, stopping_strategy)\u001b[39m\n\u001b[32m   4591\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_interleave_iterable_datasets\u001b[39m(\n\u001b[32m   4592\u001b[39m     datasets: \u001b[38;5;28mlist\u001b[39m[IterableDataset],\n\u001b[32m   4593\u001b[39m     probabilities: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4599\u001b[39m     ] = \u001b[33m\"\u001b[39m\u001b[33mfirst_exhausted\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   4600\u001b[39m ) -> IterableDataset:\n\u001b[32m   4601\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4602\u001b[39m \u001b[33;03m    Interleave several iterable datasets (sources) into a single iterable dataset.\u001b[39;00m\n\u001b[32m   4603\u001b[39m \u001b[33;03m    The new iterable dataset alternates between the sources to yield examples.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4623\u001b[39m \u001b[33;03m        `datasets.IterableDataset`\u001b[39;00m\n\u001b[32m   4624\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4625\u001b[39m     datasets = [\u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_resolve_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[32m   4627\u001b[39m     \u001b[38;5;66;03m# Perform checks\u001b[39;00m\n\u001b[32m   4628\u001b[39m     _check_if_features_can_be_aligned([dset.features \u001b[38;5;28;01mfor\u001b[39;00m dset \u001b[38;5;129;01min\u001b[39;00m datasets])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahada\\Documents\\abdulahad\\spectra\\.venv\\Lib\\site-packages\\datasets\\iterable_dataset.py:3603\u001b[39m, in \u001b[36mIterableDataset._resolve_features\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3601\u001b[39m     features = \u001b[38;5;28mself\u001b[39m._ex_iterable.features\n\u001b[32m   3602\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3603\u001b[39m     features = _infer_features_from_batch(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwith_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   3604\u001b[39m info = \u001b[38;5;28mself\u001b[39m.info.copy()\n\u001b[32m   3605\u001b[39m info.features = features\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahada\\Documents\\abdulahad\\spectra\\.venv\\Lib\\site-packages\\datasets\\iterable_dataset.py:2363\u001b[39m, in \u001b[36mIterableDataset._head\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   2362\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_head\u001b[39m(\u001b[38;5;28mself\u001b[39m, n=\u001b[32m5\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahada\\Documents\\abdulahad\\spectra\\.venv\\Lib\\site-packages\\datasets\\iterable_dataset.py:2568\u001b[39m, in \u001b[36mIterableDataset.iter\u001b[39m\u001b[34m(self, batch_size, drop_last_batch)\u001b[39m\n\u001b[32m   2565\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2567\u001b[39m iterator = \u001b[38;5;28miter\u001b[39m(ex_iterable)\n\u001b[32m-> \u001b[39m\u001b[32m2568\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If batched, first build the batch\u001b[39;49;00m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ignore last batch\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahada\\Documents\\abdulahad\\spectra\\.venv\\Lib\\site-packages\\datasets\\iterable_dataset.py:2065\u001b[39m, in \u001b[36mFormattedExamplesIterable.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2059\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2060\u001b[39m     format_dict = (\n\u001b[32m   2061\u001b[39m         formatter.recursive_tensorize\n\u001b[32m   2062\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatter, TensorFormatter)\n\u001b[32m   2063\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# cast in case features is None\u001b[39;00m\n\u001b[32m   2064\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2065\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# don't apply feature types if already applied by ex_iterable (e.g. in case of chained with_format)\u001b[39;49;00m\n\u001b[32m   2067\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_typed\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_apply_feature_types_on_example\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m                \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahada\\Documents\\abdulahad\\spectra\\.venv\\Lib\\site-packages\\datasets\\iterable_dataset.py:1252\u001b[39m, in \u001b[36mMappedExamplesIterable.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1250\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m key, formatter.format_row(pa_table)\n\u001b[32m   1251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahada\\Documents\\abdulahad\\spectra\\.venv\\Lib\\site-packages\\datasets\\iterable_dataset.py:1431\u001b[39m, in \u001b[36mMappedExamplesIterable._iter\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1425\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batched:\n\u001b[32m   1426\u001b[39m     outputs = (\n\u001b[32m   1427\u001b[39m         (key, transformed_example)\n\u001b[32m   1428\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m key, transformed_batch \u001b[38;5;129;01min\u001b[39;00m outputs\n\u001b[32m   1429\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m transformed_example \u001b[38;5;129;01min\u001b[39;00m _batch_to_examples(transformed_batch)\n\u001b[32m   1430\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1431\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_example\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1432\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprevious_state\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m   1433\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_examples_since_previous_state\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahada\\Documents\\abdulahad\\spectra\\.venv\\Lib\\site-packages\\datasets\\iterable_dataset.py:1416\u001b[39m, in \u001b[36mMappedExamplesIterable._iter.<locals>.iter_outputs\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batched:\n\u001b[32m   1415\u001b[39m         \u001b[38;5;28mself\u001b[39m._state_dict[\u001b[33m\"\u001b[39m\u001b[33mprevious_state_example_idx\u001b[39m\u001b[33m\"\u001b[39m] = current_idx\n\u001b[32m-> \u001b[39m\u001b[32m1416\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_example\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict:\n\u001b[32m   1418\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batched:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahada\\Documents\\abdulahad\\spectra\\.venv\\Lib\\site-packages\\datasets\\iterable_dataset.py:1346\u001b[39m, in \u001b[36mMappedExamplesIterable._iter.<locals>.apply_function\u001b[39m\u001b[34m(key_example, indices)\u001b[39m\n\u001b[32m   1344\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   1345\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(key_example, indices)\n\u001b[32m-> \u001b[39m\u001b[32m1346\u001b[39m processed_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1347\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(key_example, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mget_dataset_stream.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m phase == \u001b[33m\"\u001b[39m\u001b[33mphase1\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     46\u001b[39m     \u001b[38;5;66;03m# 1. LAION-5B (using laion2B-en subset for demo)\u001b[39;00m\n\u001b[32m     47\u001b[39m     ds_laion = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mlaion/laion2B-en\u001b[39m\u001b[33m\"\u001b[39m, split=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m, streaming=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     ds_laion = ds_laion.map(\u001b[38;5;28;01mlambda\u001b[39;00m x: {\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: x[\u001b[33m\"\u001b[39m\u001b[33mTEXT\u001b[39m\u001b[33m\"\u001b[39m]})\n\u001b[32m     49\u001b[39m     datasets_list.append(ds_laion)\n\u001b[32m     50\u001b[39m     probs.append(\u001b[32m0.7\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'image'"
     ]
    }
   ],
   "source": [
    "# Training Loop with Phases\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=CONFIG[\"weight_decay\"])\n",
    "model, optimizer = accelerator.prepare(model, optimizer)\n",
    "\n",
    "# Check for resumption\n",
    "start_step = 0\n",
    "consumed_samples = 0\n",
    "saved_state = load_landmark()\n",
    "if saved_state:\n",
    "    start_step = saved_state[\"global_step\"]\n",
    "    consumed_samples = saved_state[\"consumed_samples\"]\n",
    "    # Load weights\n",
    "    try:\n",
    "        accelerator.load_state(CONFIG[\"output_dir\"])\n",
    "        print(\"Weights loaded.\")\n",
    "    except Exception:\n",
    "        print(\"Warning: State file found but weights could not be loaded from local dir. Pulling from Hub if possible.\")\n",
    "\n",
    "# Define Phases\n",
    "phases = [\"phase1\", \"phase2\"]\n",
    "current_phase_idx = 0\n",
    "if start_step > CONFIG[\"max_steps\"] // 2:\n",
    "    current_phase_idx = 1 # Simple logic to switch phases halfway\n",
    "\n",
    "dataset = get_dataset_stream(phases[current_phase_idx])\n",
    "dataset = dataset.shuffle(buffer_size=1000, seed=42)\n",
    "dataset = dataset.skip(consumed_samples) # Skip processed samples\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=CONFIG[\"batch_size\"], collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
    "train_dataloader = accelerator.prepare(train_dataloader)\n",
    "\n",
    "model.train()\n",
    "progress_bar = tqdm(range(start_step, CONFIG[\"max_steps\"]))\n",
    "global_step = start_step\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    if batch is None:\n",
    "        continue\n",
    "    \n",
    "    with accelerator.accumulate(model):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    if accelerator.sync_gradients:\n",
    "        progress_bar.update(1)\n",
    "        global_step += 1\n",
    "        consumed_samples += CONFIG[\"batch_size\"] * CONFIG[\"grad_accum_steps\"] * accelerator.num_processes\n",
    "        \n",
    "        if global_step % 100 == 0:\n",
    "            accelerator.print(f\"Step {global_step}: Loss {loss.item()}\")\n",
    "            \n",
    "        if global_step % CONFIG[\"checkpoint_interval\"] == 0:\n",
    "            accelerator.wait_for_everyone()\n",
    "            if accelerator.is_main_process:\n",
    "                accelerator.save_state(CONFIG[\"output_dir\"])\n",
    "                save_landmark(global_step, consumed_samples)\n",
    "                \n",
    "                if CONFIG[\"push_to_hub\"]:\n",
    "                     model.push_to_hub(CONFIG[\"hub_model_id\"])\n",
    "                     CLIPProcessor.from_pretrained(CONFIG[\"model_name\"]).push_to_hub(CONFIG[\"hub_model_id\"])\n",
    "\n",
    "    # Phase Switch Logic\n",
    "    if global_step == CONFIG[\"max_steps\"] // 2 and current_phase_idx == 0:\n",
    "        print(\"Switching to Phase 2...\")\n",
    "        current_phase_idx = 1\n",
    "        dataset = get_dataset_stream(\"phase2\")\n",
    "        # Re-wrap dataloader\n",
    "        train_dataloader = DataLoader(dataset, batch_size=CONFIG[\"batch_size\"], collate_fn=collate_fn, num_workers=4)\n",
    "        train_dataloader = accelerator.prepare(train_dataloader)\n",
    "\n",
    "    if global_step >= CONFIG[\"max_steps\"]:\n",
    "        break\n",
    "\n",
    "print(\"Training Complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "object-detection (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
