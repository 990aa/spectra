{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "872cd5ce",
   "metadata": {},
   "source": [
    "# Spectra: Object Recognition Model Training\n",
    "\n",
    "This notebook implements the training pipeline for 'spectra', trained on LAION-5B, WIT, PMD, and expanded detection datasets (COCO, OpenImages, etc.) using CLIP-style contrastive learning.\n",
    "\n",
    "## Features\n",
    "- **Backbone**: `laion/CLIP-ViT-B-16-laion2B-s34B-b88K` (OpenCLIP ViT-B/16)\n",
    "- **Streaming Data**: Zero-disk usage for massive datasets.\n",
    "- **Robust Checkpointing**: Resumable training from HF Hub (Landmarks).\n",
    "- **Phase 2 Training**: Includes COCO, OpenImages, Objects365, VOC, LVIS, Visual Genome.\n",
    "- **Optimizations**: Mixed Precision (FP16), Gradient Accumulation, Torch Compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4927340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: .env file not found in current directory\n",
      "Logged in to HF with token ending in ...nrRd\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, interleave_datasets\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from accelerate import Accelerator\n",
    "from huggingface_hub import login, HfApi, hf_hub_download\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables\n",
    "# add env path here\n",
    "env_path = Path(\".env\")\n",
    "\n",
    "if env_path.exists():\n",
    "    load_dotenv(dotenv_path=env_path, override=True)\n",
    "    print(\"Loaded .env file\")\n",
    "else:\n",
    "    print(\"Warning: .env file not found in current directory\")\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(f\"Logged in to HF with token ending in ...{HF_TOKEN[-4:]}\")\n",
    "else:\n",
    "    print(\"WARNING: HF_TOKEN not found in .env. Please set it for Hub uploads.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6bf689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerator setup: cuda, Mixed Precision: fp16\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"model_name\": \"laion/CLIP-ViT-B-16-laion2B-s34B-b88K\",  # ViT-B/16 Backbone\n",
    "    \"fallback_model\": \"openai/clip-vit-base-patch32\",\n",
    "    \"output_dir\": \"./spectra_checkpoints\",\n",
    "    \"hub_model_id\": \"990aa/spectra\",\n",
    "    \"batch_size\": 64,  # Per device\n",
    "    \"grad_accum_steps\": 4,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"max_steps\": 100000,  # Increased for multi-phase\n",
    "    \"warmup_steps\": 2000,\n",
    "    \"mixed_precision\": \"fp16\",\n",
    "    \"image_size\": 224,\n",
    "    \"push_to_hub\": True,\n",
    "    \"checkpoint_interval\": 1000,\n",
    "    \"state_file\": \"training_state.json\",\n",
    "    \"model_card_file\": \"README.md\",\n",
    "}\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision=CONFIG[\"mixed_precision\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"grad_accum_steps\"],\n",
    "    log_with=\"all\",\n",
    "    project_dir=\"logs\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Accelerator setup: {accelerator.device}, Mixed Precision: {accelerator.mixed_precision}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2867204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpointing & Resumption Logic\n",
    "def save_landmark(step: int, consumed_samples: int) -> None:\n",
    "    \"\"\"\n",
    "    Save a training checkpoint (landmark) and upload it to the Hugging Face Hub.\n",
    "\n",
    "    Args:\n",
    "        step (int): The current global training step.\n",
    "        consumed_samples (int): The total number of samples consumed so far.\n",
    "    \"\"\"\n",
    "    if not accelerator.is_main_process:\n",
    "        return\n",
    "\n",
    "    state = {\"global_step\": step, \"consumed_samples\": consumed_samples}\n",
    "    with open(CONFIG[\"state_file\"], \"w\") as f:\n",
    "        json.dump(state, f)\n",
    "\n",
    "    # Upload to HF\n",
    "    api = HfApi()\n",
    "    try:\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=CONFIG[\"state_file\"],\n",
    "            path_in_repo=CONFIG[\"state_file\"],\n",
    "            repo_id=CONFIG[\"hub_model_id\"],\n",
    "            repo_type=\"model\",\n",
    "        )\n",
    "        print(f\"Landmark saved at step {step}\")\n",
    "\n",
    "        # Upload model card\n",
    "        if os.path.exists(CONFIG[\"model_card_file\"]):\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=CONFIG[\"model_card_file\"],\n",
    "                path_in_repo=\"README.md\",\n",
    "                repo_id=CONFIG[\"hub_model_id\"],\n",
    "                repo_type=\"model\",\n",
    "            )\n",
    "            print(\"Model card uploaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload to Hub: {e}\")\n",
    "\n",
    "\n",
    "def load_landmark() -> dict | None:\n",
    "    \"\"\"\n",
    "    Load the latest checkpoint state from the Hugging Face Hub.\n",
    "\n",
    "    Returns:\n",
    "        dict | None: The loaded state dictionary if found, else None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path = hf_hub_download(\n",
    "            repo_id=CONFIG[\"hub_model_id\"], filename=CONFIG[\"state_file\"]\n",
    "        )\n",
    "        with open(path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        print(f\"Found previous state: Step {state['global_step']}\")\n",
    "        return state\n",
    "    except Exception:\n",
    "        print(\"No previous state found. Starting from scratch.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a0745b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pipeline Construction\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def get_dataset_stream(phase: str = \"phase1\") -> object:\n",
    "    \"\"\"\n",
    "    Initialize and return a streaming dataset for the specified phase.\n",
    "\n",
    "    Args:\n",
    "        phase (str): The training phase ('phase1' or 'phase2').\n",
    "\n",
    "    Returns:\n",
    "        object: An interleaved streaming dataset.\n",
    "    \"\"\"\n",
    "    print(f\"Initializing Data Stream for {phase}...\")\n",
    "\n",
    "    datasets_list = []\n",
    "    probs = []\n",
    "\n",
    "    if phase == \"phase1\":\n",
    "        # 1. LAION-5B (using laion2B-en subset for demo)\n",
    "        try:\n",
    "            ds_laion = load_dataset(\"laion/laion2B-en\", split=\"train\", streaming=True)\n",
    "            \n",
    "            # Select only the columns we need and rename them\n",
    "            ds_laion = ds_laion.select_columns(['URL', 'TEXT'])\n",
    "            ds_laion = ds_laion.rename_columns({'URL': 'image', 'TEXT': 'text'})\n",
    "            \n",
    "            # Filter out invalid entries\n",
    "            ds_laion = ds_laion.filter(\n",
    "                lambda x: x.get('image') and x.get('text') and \n",
    "                isinstance(x['image'], str) and isinstance(x['text'], str)\n",
    "            )\n",
    "            \n",
    "            datasets_list.append(ds_laion)\n",
    "            probs.append(0.5)\n",
    "            print(\"✓ LAION-2B loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load LAION-2B: {e}\")\n",
    "\n",
    "        # 2. WIT (Wikipedia Image Text)\n",
    "        try:\n",
    "            ds_wit = load_dataset(\"wikimedia/wit_base\", split=\"train\", streaming=True)\n",
    "            \n",
    "            # Extract image_url and caption fields\n",
    "            def wit_transform(x):\n",
    "                return {\n",
    "                    'image': x.get('image_url', ''),\n",
    "                    'text': x.get('caption_reference_description', '') or x.get('caption', '')\n",
    "                }\n",
    "            \n",
    "            ds_wit = ds_wit.map(wit_transform, remove_columns=ds_wit.column_names)\n",
    "            ds_wit = ds_wit.filter(\n",
    "                lambda x: x.get('image') and x.get('text') and \n",
    "                isinstance(x['image'], str) and isinstance(x['text'], str)\n",
    "            )\n",
    "            \n",
    "            datasets_list.append(ds_wit)\n",
    "            probs.append(0.3)\n",
    "            print(\"✓ WIT loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load WIT: {e}\")\n",
    "\n",
    "        # 3. Conceptual Captions\n",
    "        try:\n",
    "            ds_cc = load_dataset(\n",
    "                \"google-research-datasets/conceptual_captions\",\n",
    "                split=\"train\",\n",
    "                streaming=True,\n",
    "            )\n",
    "            \n",
    "            ds_cc = ds_cc.rename_columns({'image_url': 'image'})\n",
    "            ds_cc = ds_cc.select_columns(['image', 'caption'])\n",
    "            ds_cc = ds_cc.rename_columns({'caption': 'text'})\n",
    "            \n",
    "            ds_cc = ds_cc.filter(\n",
    "                lambda x: x.get('image') and x.get('text') and \n",
    "                isinstance(x['image'], str) and isinstance(x['text'], str)\n",
    "            )\n",
    "            \n",
    "            datasets_list.append(ds_cc)\n",
    "            probs.append(0.2)\n",
    "            print(\"✓ Conceptual Captions loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load Conceptual Captions: {e}\")\n",
    "\n",
    "    elif phase == \"phase2\":\n",
    "        # For Phase 2, we'll use Phase 1 datasets as COCO has PIL images that cause issues\n",
    "        # You can add proper Phase 2 datasets that provide URLs instead of PIL images\n",
    "        print(\"Phase 2: Using URL-based datasets (COCO with PIL images skipped)\")\n",
    "        return get_dataset_stream(\"phase1\")\n",
    "\n",
    "    # If no datasets loaded at all\n",
    "    if not datasets_list:\n",
    "        print(\"ERROR: No datasets could be loaded.\")\n",
    "        raise RuntimeError(\n",
    "            \"No datasets available. Please check your internet connection and dataset access permissions.\"\n",
    "        )\n",
    "\n",
    "    # Normalize probabilities\n",
    "    probs = [p / sum(probs) for p in probs]\n",
    "\n",
    "    print(f\"Interleaving {len(datasets_list)} datasets with probabilities: {probs}\")\n",
    "\n",
    "    combined_ds = interleave_datasets(\n",
    "        datasets_list, probabilities=probs, seed=42, stopping_strategy=\"first_exhausted\"\n",
    "    )\n",
    "\n",
    "    return combined_ds\n",
    "\n",
    "\n",
    "def collate_fn(batch: list) -> dict | None:\n",
    "    \"\"\"\n",
    "    Collate function to process a batch of items with robust image loading.\n",
    "\n",
    "    Args:\n",
    "        batch (list): List of dataset items.\n",
    "\n",
    "    Returns:\n",
    "        dict | None: Processed batch tensors or None if empty.\n",
    "    \"\"\"\n",
    "    processor = CLIPProcessor.from_pretrained(CONFIG[\"model_name\"])\n",
    "    texts = []\n",
    "    images = []\n",
    "\n",
    "    for item in batch:\n",
    "        if item is None or not isinstance(item, dict):\n",
    "            continue\n",
    "\n",
    "        img = item.get(\"image\")\n",
    "        txt = item.get(\"text\")\n",
    "\n",
    "        if not img or not txt:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Handle different image formats\n",
    "            if isinstance(img, str):\n",
    "                # URL or file path\n",
    "                if img.startswith(('http://', 'https://')):\n",
    "                    # Download from URL with retry\n",
    "                    try:\n",
    "                        response = requests.get(\n",
    "                            img, \n",
    "                            timeout=5, \n",
    "                            headers={'User-Agent': 'Mozilla/5.0'}\n",
    "                        )\n",
    "                        response.raise_for_status()\n",
    "                        img = Image.open(BytesIO(response.content))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                else:\n",
    "                    # Local file path\n",
    "                    try:\n",
    "                        img = Image.open(img)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "            elif isinstance(img, dict):\n",
    "                # HuggingFace datasets format with bytes\n",
    "                if 'bytes' in img:\n",
    "                    try:\n",
    "                        img = Image.open(BytesIO(img['bytes']))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                elif 'path' in img:\n",
    "                    try:\n",
    "                        img = Image.open(img['path'])\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "            elif hasattr(img, 'convert'):\n",
    "                # Already a PIL Image\n",
    "                pass\n",
    "            else:\n",
    "                # Skip if we can't handle it\n",
    "                continue\n",
    "\n",
    "            # Convert to RGB\n",
    "            if hasattr(img, 'mode'):\n",
    "                if img.mode != \"RGB\":\n",
    "                    img = img.convert(\"RGB\")\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Validate image\n",
    "            if hasattr(img, 'size'):\n",
    "                width, height = img.size\n",
    "                if width < 10 or height < 10:  # Skip tiny images\n",
    "                    continue\n",
    "\n",
    "            images.append(img)\n",
    "            texts.append(str(txt)[:77])  # Truncate for CLIP\n",
    "            \n",
    "        except Exception:\n",
    "            # Skip problematic images\n",
    "            continue\n",
    "\n",
    "    if not images:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        return processor(\n",
    "            text=texts, \n",
    "            images=images, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True\n",
    "        )\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92fc30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load laion/CLIP-ViT-B-16-laion2B-s34B-b88K, falling back to openai/clip-vit-base-patch32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddae978279e4069adfaf6c846f6693f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d483aeb8d6fd4e3ab60a1b62a97b681f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6280ad070549a38b8a8a15a9830619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Model Setup\n",
    "try:\n",
    "    model = CLIPModel.from_pretrained(CONFIG[\"model_name\"])\n",
    "    print(f\"Loaded {CONFIG['model_name']}\")\n",
    "except Exception:\n",
    "    print(\n",
    "        f\"Failed to load {CONFIG['model_name']}, falling back to {CONFIG['fallback_model']}\"\n",
    "    )\n",
    "    model = CLIPModel.from_pretrained(CONFIG[\"fallback_model\"])\n",
    "    CONFIG[\"model_name\"] = CONFIG[\"fallback_model\"]\n",
    "\n",
    "# Freeze early layers\n",
    "for name, param in model.vision_model.encoder.layers[:6].named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Torch Compile\n",
    "try:\n",
    "    model = torch.compile(model)\n",
    "    print(\"Model compiled.\")\n",
    "except Exception:\n",
    "    print(\"torch.compile skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38efa996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous state found. Starting from scratch.\n",
      "Initializing Data Stream for phase1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d35a05f715f4576a2380916653c7734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load LAION-2B: Column name ['TEXT', 'URL'] not in the dataset. Columns in the dataset: ['url', 'similarity', 'hash', 'pwatermark', 'punsafe', 'caption', 'key', 'status', 'error_message', 'width', 'height', 'original_width', 'original_height', 'exif', 'md5'].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb66e80fb3f4c42a0fdb18fe9e7aef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d82bd0b87648369cd4288145ae73d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8723a3c8f44613a78d13c3777e57c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ WIT loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c888632c65c4f62940fddc6dc4815b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Conceptual Captions loaded\n",
      "Interleaving 2 datasets with probabilities: [0.6, 0.4]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Training Loop with Phases\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=CONFIG[\"weight_decay\"]\n",
    ")\n",
    "model, optimizer = accelerator.prepare(model, optimizer)\n",
    "\n",
    "# Check for resumption\n",
    "start_step = 0\n",
    "consumed_samples = 0\n",
    "saved_state = load_landmark()\n",
    "if saved_state:\n",
    "    start_step = saved_state[\"global_step\"]\n",
    "    consumed_samples = saved_state[\"consumed_samples\"]\n",
    "    # Load weights\n",
    "    try:\n",
    "        accelerator.load_state(CONFIG[\"output_dir\"])\n",
    "        print(\"Weights loaded.\")\n",
    "    except Exception:\n",
    "        print(\n",
    "            \"Warning: State file found but weights could not be loaded from local dir. Pulling from Hub if possible.\"\n",
    "        )\n",
    "\n",
    "# Define Phases\n",
    "phases = [\"phase1\", \"phase2\"]\n",
    "current_phase_idx = 0\n",
    "if start_step > CONFIG[\"max_steps\"] // 2:\n",
    "    current_phase_idx = 1  # Simple logic to switch phases halfway\n",
    "\n",
    "dataset = get_dataset_stream(phases[current_phase_idx])\n",
    "dataset = dataset.shuffle(buffer_size=1000, seed=42)\n",
    "dataset = dataset.skip(consumed_samples)  # Skip processed samples\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "train_dataloader = accelerator.prepare(train_dataloader)\n",
    "\n",
    "model.train()\n",
    "progress_bar = tqdm(range(start_step, CONFIG[\"max_steps\"]))\n",
    "global_step = start_step\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    if batch is None:\n",
    "        continue\n",
    "\n",
    "    with accelerator.accumulate(model):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if accelerator.sync_gradients:\n",
    "        progress_bar.update(1)\n",
    "        global_step += 1\n",
    "        consumed_samples += (\n",
    "            CONFIG[\"batch_size\"]\n",
    "            * CONFIG[\"grad_accum_steps\"]\n",
    "            * accelerator.num_processes\n",
    "        )\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            accelerator.print(f\"Step {global_step}: Loss {loss.item()}\")\n",
    "\n",
    "        if global_step % CONFIG[\"checkpoint_interval\"] == 0:\n",
    "            accelerator.wait_for_everyone()\n",
    "            if accelerator.is_main_process:\n",
    "                accelerator.save_state(CONFIG[\"output_dir\"])\n",
    "                save_landmark(global_step, consumed_samples)\n",
    "\n",
    "                if CONFIG[\"push_to_hub\"]:\n",
    "                    model.push_to_hub(CONFIG[\"hub_model_id\"])\n",
    "                    CLIPProcessor.from_pretrained(CONFIG[\"model_name\"]).push_to_hub(\n",
    "                        CONFIG[\"hub_model_id\"]\n",
    "                    )\n",
    "\n",
    "    # Phase Switch Logic\n",
    "    if global_step == CONFIG[\"max_steps\"] // 2 and current_phase_idx == 0:\n",
    "        print(\"Switching to Phase 2...\")\n",
    "        current_phase_idx = 1\n",
    "        dataset = get_dataset_stream(\"phase2\")\n",
    "        # Re-wrap dataloader\n",
    "        train_dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=CONFIG[\"batch_size\"],\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=4,\n",
    "        )\n",
    "        train_dataloader = accelerator.prepare(train_dataloader)\n",
    "\n",
    "    if global_step >= CONFIG[\"max_steps\"]:\n",
    "        break\n",
    "\n",
    "print(\"Training Complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "object-detection (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
